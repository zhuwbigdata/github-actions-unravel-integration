{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.devops.connection import Connection\n",
    "from azure.devops.v7_1.git.models import GitPullRequest\n",
    "from azure.devops.v7_1.git.models import CommentThread, Comment\n",
    "from msrest.authentication import BasicAuthentication\n",
    "\n",
    "import sys\n",
    "from datetime import timedelta, datetime\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import getopt\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBRKS URL pattern\n",
    "pattern = r\"^https://adb-([0-9]+).([0-9]+).azuredatabricks.net/\\?o=([0-9]+)#job/([0-9]+)/run/([0-9]+)$\"\n",
    "pattern_as_text = r\"https://adb-([0-9]+).([0-9]+).azuredatabricks.net/\\?o=([0-9]+)#job/([0-9]+)/run/([0-9]+)\"  \n",
    "cleanRe = re.compile('<.*?>')\n",
    "\n",
    "app_summary_map = {}\n",
    "app_summary_map_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api(api_url, api_token):\n",
    "    response = requests.get(api_url,\n",
    "                            verify=False,\n",
    "                            headers={'Authorization': api_token})\n",
    "    json_obj = json.loads(response.content)\n",
    "    return json_obj\n",
    "\n",
    "def check_response_on_get(json_val):\n",
    "    if 'message' in json_val :\n",
    "        if json_val['message'] == 'INVALID_TOKEN' :\n",
    "           raise ValueError('INVALID_TOKEN')\n",
    "\n",
    "def post_api(api_url, api_token, kv_dict):\n",
    "    response = requests.post(api_url,\n",
    "                             data = json.dumps(kv_dict),\n",
    "                             verify=False,\n",
    "                             headers={'Authorization': api_token,\n",
    "                                      'accept': 'application/json',\n",
    "                                      'Content-Type': 'application/json'})\n",
    "    json_obj = json.loads(response.content)\n",
    "    return json_obj\n",
    "\n",
    "def check_response_on_post(json_val):\n",
    "    if 'message' in json_val :\n",
    "        if json_val['message'] == 'INVALID_TOKEN' :\n",
    "           raise ValueError('INVALID_TOKEN')\n",
    "    elif len(json_val) == 0:\n",
    "        print(json_val)\n",
    "        raise ValueError('Response is empty') \n",
    "    elif 'results' not in json_val:\n",
    "        print(json_val)\n",
    "        raise ValueError('KEY results NOT FOUND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_job_id(base_url, api_token, start_time, end_time, job_id):\n",
    "    api_url = base_url + \\\n",
    "              '/api/v1/ds/api/v1/databricks/runs'\n",
    "    kv_pairs = {\"from\":0,\n",
    "                \"appTypes\":[\"db\"],\n",
    "                \"appStatus\":[\"K\",\"F\",\"R\",\"S\",\"P\",\"U\",\"W\"],\n",
    "                \"size\":\"1000\",\n",
    "                \"start_time\":start_time,\n",
    "                \"end_time\":end_time,\n",
    "                \"jobIds\": [ job_id ],\n",
    "                \"sort\": [{\"startTime\": {\"order\": \"desc\"}}],\n",
    "                \"queryOnFinishedTime\": False,\n",
    "                }\n",
    "    print(\"URL: \" + api_url)\n",
    "    print(kv_pairs)\n",
    "    json_val  = post_api(api_url, api_token, kv_pairs)\n",
    "    check_response_on_post(json_val)\n",
    "    return json_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_job_id(base_url, api_token, start_time, end_time, job_id):\n",
    "    api_url = base_url + \\\n",
    "              '/api/v1/ds/api/v1/databricks/runs'\n",
    "    kv_pairs = {\"from\":0,\n",
    "                \"appTypes\":[\"db\"],\n",
    "                \"appStatus\":[\"K\",\"F\",\"R\",\"S\",\"P\",\"U\",\"W\"],\n",
    "                \"size\":\"1000\",\n",
    "                \"start_time\":start_time,\n",
    "                \"end_time\":end_time,\n",
    "                \"jobIds\": [ job_id ],\n",
    "                \"sort\": [{\"startTime\": {\"order\": \"desc\"}}],\n",
    "                \"queryOnFinishedTime\": False,\n",
    "                }\n",
    "    print(\"URL: \" + api_url)\n",
    "    print(kv_pairs)\n",
    "    json_val  = post_api(api_url, api_token, kv_pairs)\n",
    "    check_response_on_post(json_val)\n",
    "    return json_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_globalsearchpattern(base_url, api_token, start_time, end_time, gsp):\n",
    "    api_url = base_url + \\\n",
    "              '/api/v1/ds/api/v1/databricks/runs'\n",
    "    kv_pairs = {\"from\":0,\n",
    "                \"appTypes\":[\"db\"],\n",
    "                \"appStatus\":[\"K\",\"F\",\"R\",\"S\",\"P\",\"U\",\"W\"],\n",
    "                \"size\":\"1\",\n",
    "                \"start_time\":start_time,\n",
    "                \"end_time\":end_time,\n",
    "                \"globalsearchpattern\": gsp,\n",
    "                \"sort\": [{\"startTime\": {\"order\": \"desc\"}}],\n",
    "                \"queryOnFinishedTime\": False,\n",
    "                }\n",
    "    print(\"URL: \" + api_url)\n",
    "    print(kv_pairs)\n",
    "    json_val  = post_api(api_url, api_token, kv_pairs)\n",
    "    check_response_on_post(json_val)\n",
    "    return json_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_summary_by_globalsearchpattern(base_url, api_token, start_time, end_time, gsp):\n",
    "    api_url = base_url + \\\n",
    "              '/api/v1/ds/api/v1/databricks/runs/' + gsp + '/tasks/summary'\n",
    "    print(\"URL: \" + api_url)\n",
    "    json_val  = get_api(api_url, api_token)\n",
    "    check_response_on_get(json_val)\n",
    "    return json_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_analysis(base_url, api_token, clusterUId, id):\n",
    "    api_url = base_url + '/api/v1/spark/' + clusterUId + '/' + id + '/analysis'\n",
    "    print(\"URL: \" + api_url)\n",
    "    json_val  = get_api(api_url, api_token)\n",
    "    check_response_on_get(json_val)\n",
    "    return json_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_summary(base_url, api_token, clusterUId, id):\n",
    "    api_url = base_url + \"/api/v1/spark/\" + clusterUId + \"/\" + id + \"/appsummary\"\n",
    "    print(\"URL: \" + api_url)\n",
    "    json_val = get_api(api_url, api_token)\n",
    "    check_response_on_get(json_val)\n",
    "    return json_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCurrentDateTime():   \n",
    "    today = datetime.today()\n",
    "    return  datetime(year=today.year, month=today.month, day=today.day, hour=today.hour, second=today.second) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fill in with your personal access token and org URL\n",
    "#personal_access_token = 'nvc4yu4wwu257m4fsk7unpopbjwjcrtksjxdfskdmvvwmuxx5bcq'\n",
    "#organization_url = 'https://dev.azure.com/unraveldevops'\n",
    "#project_name = 'DBRK-CICD'\n",
    "#build_id = 88\n",
    "\n",
    "def search_dbrks_job_runs(connection, project_name, build_id):\n",
    "\n",
    "  # build client\n",
    "  build_client = connection.clients.get_build_client()\n",
    "\n",
    "  build = build_client.get_build(project_name, build_id)\n",
    "  build_json = build.as_dict()\n",
    "\n",
    "  print(\"reason:\" + build_json['reason'])\n",
    "  print(\"pr:\" + build_json['trigger_info']['pr.number'])\n",
    "  print(\"repo_id:\" + build_json['repository']['id'])\n",
    "          \n",
    "  #with open(\"build.json\", \"w\") as outfile:\n",
    "  #    json.dump(build.as_dict(), outfile)\n",
    "\n",
    "\n",
    "  # git client\n",
    "  git_client = connection.clients.get_git_client()\n",
    "  pr = git_client.get_pull_request_by_id(build_json['trigger_info']['pr.number'])\n",
    "  pr_json = pr.as_dict()\n",
    "  pr_id = pr_json['pull_request_id']\n",
    "  runs_json = json.loads(pr_json['description'])\n",
    "  #print(runs_json)\n",
    "\n",
    "  job_run_list = []\n",
    "  for run_url in runs_json['runs']:\n",
    "      match = re.search(pattern, run_url)\n",
    "      if match:\n",
    "        workspace_id = match.group(3)\n",
    "        job_id = match.group(4)\n",
    "        run_id = match.group(5)\n",
    "        job_run_list.append({'pr_id': pr_id, 'pdbrks_url': run_url, 'workspace_id': workspace_id, 'job_id': job_id, 'run_id': run_id})\n",
    "  \n",
    "  return job_run_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_devops_build(connection, project_name, build_id):\n",
    "  # build client\n",
    "  build_client = connection.clients.get_build_client()\n",
    "  build = build_client.get_build(project_name, build_id)\n",
    "\n",
    "  return build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_pr(connection, pr_number):\n",
    "  # git client\n",
    "  git_client = connection.clients.get_git_client()\n",
    "  pr = git_client.get_pull_request_by_id(pr_number)\n",
    "\n",
    "  return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_git_pr(connection, pr, repository_id, pr_number, project_name):\n",
    "  # git client\n",
    "  git_client = connection.clients.get_git_client()\n",
    "  pr = git_client.update_pull_request(pr, repository_id, pr_number, project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pr_comments(connection, repository_id, pr_number, comment_thread):\n",
    "  # git client\n",
    "  git_client = connection.clients.get_git_client()\n",
    "  git_client.create_thread(comment_thread, repository_id, pr_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_runs_from_description(pr_id, description_json):\n",
    "  job_run_list = []\n",
    "  for run_url in description_json['runs']:\n",
    "      match = re.search(pattern, run_url)\n",
    "      if match:\n",
    "        print(run_url)\n",
    "        workspace_id = match.group(3)\n",
    "        job_id = match.group(4)\n",
    "        run_id = match.group(5)\n",
    "        job_run_list.append({'pr_id': pr_id, 'pdbrks_url': run_url, 'workspace_id': workspace_id, 'job_id': job_id, 'run_id': run_id})\n",
    "  \n",
    "  return job_run_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_runs_from_description_as_text(pr_id, description_text):\n",
    "  job_run_list = []\n",
    "  print(\"Description:\\n\" + description_text)\n",
    "  print(\"Patten: \" + pattern_as_text)\n",
    "  matches = re.findall(pattern_as_text, description_text)\n",
    "  if matches:\n",
    "    for match in matches:\n",
    "      workspace_id = match[2]\n",
    "      job_id = match[3]\n",
    "      run_id = match[4]\n",
    "      job_run_list.append({'pr_id': pr_id, 'workspace_id': workspace_id, 'job_id': job_id, 'run_id': run_id})\n",
    "  else:\n",
    "    print(\"no match\")\n",
    "  return job_run_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_organization_connection(organization_url, personal_access_token):\n",
    "  credentials = BasicAuthentication('', personal_access_token)\n",
    "  connection = Connection(base_url=organization_url, creds=credentials)\n",
    "  return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comments_with_markdown(job_run_result_list):\n",
    "    comments = \"\"\n",
    "    if job_run_result_list:\n",
    "        for r in job_run_result_list:\n",
    "            comments += \"----\\n\"\n",
    "            comments += \"<details>\\n\"\n",
    "            # comments += \"<img src='https://www.unraveldata.com/wp-content/themes/unravel-child/src/images/unLogo.svg' alt='Logo'>\\n\\n\"\n",
    "            comments += \"<summary> <img src='https://www.unraveldata.com/wp-content/themes/unravel-child/src/images/unLogo.svg' alt='Logo'> Job Id: {}, Run Id: {}</summary>\\n\\n\".format(\n",
    "                r[\"job_id\"], r[\"run_id\"]\n",
    "            )\n",
    "            comments += \"#### Workspace Id:\" + r[\"workspace_id\"] + \"\\n\"\n",
    "            comments += \"#### Job Id:\" + r[\"job_id\"] + \"\\n\"\n",
    "            comments += \"#### Run Id:\" + r[\"run_id\"] + \"\\n\"\n",
    "            comments += \"----\\n\"\n",
    "            comments += \"#### [{}]({})\\n\".format('Unravel url', r[\"unravel_url\"])\n",
    "            if r['app_summary']:\n",
    "                # Get all unique keys from the dictionaries while preserving the order\n",
    "                headers = []\n",
    "                for key in r['app_summary'].keys():\n",
    "                    if key not in headers:\n",
    "                        headers.append(key)\n",
    "\n",
    "                # Generate the header row\n",
    "                header_row = \"| \" + \" | \".join(headers) + \" |\"\n",
    "\n",
    "                # Generate the separator row\n",
    "                separator_row = \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\"\n",
    "\n",
    "                # Generate the data rows\n",
    "                data_rows = \"\\n\".join(\n",
    "                    [\n",
    "                        \"| \" + \" | \".join(str(r['app_summary'].get(h, \"\")) for h in headers)\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                # Combine the header, separator, and data rows\n",
    "                comments += \"----\\n\"\n",
    "                comments += \"# App Summary\\n\"\n",
    "                comments += \"----\\n\"\n",
    "                comments += header_row + \"\\n\" + separator_row + \"\\n\" + data_rows + \"\\n\"\n",
    "            if r[\"unravel_insights\"]:\n",
    "                comments += \"----\\n\"\n",
    "                comments += \"## Unravel Insights\\n\"\n",
    "                for insight in r[\"unravel_insights\"]:\n",
    "                    categories = insight[\"categories\"]\n",
    "                    if categories:\n",
    "                        for k in categories.keys():\n",
    "                            instances = categories[k][\"instances\"]\n",
    "                            if instances:\n",
    "                                for i in instances:\n",
    "                                    if i[\"key\"].upper() != \"SPARKAPPTIMEREPORT\":\n",
    "                                        comments += (\n",
    "                                            \"#### \"\n",
    "                                            + i[\"key\"].upper()\n",
    "                                            + \": \"\n",
    "                                            + i[\"title\"]\n",
    "                                            + \"\\n\"\n",
    "                                        )\n",
    "                                        comments += \"##### EVENT: \" + i[\"events\"] + \"\\n\"\n",
    "                                        comments += (\n",
    "                                            \"##### ACTIONS: \" + i[\"actions\"] + \"\\n\"\n",
    "                                        )\n",
    "            comments += \"</details>\\n\\n\"\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_app_summary(unravel_url, unravel_token, clusterUId, appId):\n",
    "    app_summary_map = {}\n",
    "    autoscale_dict = {}\n",
    "    summary_dict = search_summary(unravel_url, unravel_token, clusterUId, appId)\n",
    "    summary_dict = summary_dict[\"annotation\"]\n",
    "    url = '{}/#/app/application/spark?execId={}&clusterUid={}'.format(unravel_url,appId,clusterUId)\n",
    "    app_summary_map[\"Spark App\"] = '[{}]({})'.format(appId, url)\n",
    "    app_summary_map[\"Cluster\"] = clusterUId\n",
    "    app_summary_map[\"Total cost\"] = '${}'.format(summary_dict[\"cents\"] + summary_dict[\"dbuCost\"])\n",
    "    runinfo = json.loads(summary_dict[\"runInfo\"])\n",
    "    app_summary_map[\"Executor Node Type\"] = runinfo[\"node_type_id\"]\n",
    "    app_summary_map[\"Driver Node Type\"] = runinfo[\"driver_node_type_id\"]\n",
    "    app_summary_map[\"Tags\"] = runinfo[\"default_tags\"]\n",
    "    if 'custom_tags' in runinfo.keys():\n",
    "        app_summary_map[\"Tags\"] = {**app_summary_map[\"Tags\"], **runinfo[\"default_tags\"]}\n",
    "    if \"autoscale\" in runinfo.keys():\n",
    "        autoscale_dict[\"autoscale_min_workers\"] = runinfo[\"autoscale\"][\"min_workers\"]\n",
    "        autoscale_dict[\"autoscale_max_workers\"] = runinfo[\"autoscale\"][\"max_workers\"]\n",
    "        autoscale_dict[\"autoscale_target_workers\"] = runinfo[\"autoscale\"][\n",
    "            \"target_workers\"\n",
    "        ]\n",
    "        app_summary_map['Autoscale'] = autoscale_dict\n",
    "    else:\n",
    "        app_summary_map['Autoscale'] = 'Autoscale is not enabled.'\n",
    "    return app_summary_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_app_summary(unravel_url, unravel_token, clusterUId, appId):\n",
    "    app_summary_map = {}\n",
    "    autoscale_dict = {}\n",
    "    summary_dict = search_summary(unravel_url, unravel_token, clusterUId, appId)\n",
    "    summary_dict = summary_dict[\"annotation\"]\n",
    "    url = '{}/#/app/application/spark?execId={}&clusterUid={}'.format(unravel_url,appId,clusterUId)\n",
    "    app_summary_map[\"Spark App\"] = '[{}]({})'.format(appId, url)\n",
    "    app_summary_map[\"Cluster\"] = clusterUId\n",
    "    app_summary_map[\"Total cost\"] = '${}'.format(summary_dict[\"cents\"] + summary_dict[\"dbuCost\"])\n",
    "    runinfo = json.loads(summary_dict[\"runInfo\"])\n",
    "    app_summary_map[\"Executor Node Type\"] = runinfo[\"node_type_id\"]\n",
    "    app_summary_map[\"Driver Node Type\"] = runinfo[\"driver_node_type_id\"]\n",
    "    app_summary_map[\"Tags\"] = runinfo[\"default_tags\"]\n",
    "    if 'custom_tags' in runinfo.keys():\n",
    "        app_summary_map[\"Tags\"] = {**app_summary_map[\"Tags\"], **runinfo[\"default_tags\"]}\n",
    "    if \"autoscale\" in runinfo.keys():\n",
    "        autoscale_dict[\"autoscale_min_workers\"] = runinfo[\"autoscale\"][\"min_workers\"]\n",
    "        autoscale_dict[\"autoscale_max_workers\"] = runinfo[\"autoscale\"][\"max_workers\"]\n",
    "        autoscale_dict[\"autoscale_target_workers\"] = runinfo[\"autoscale\"][\n",
    "            \"target_workers\"\n",
    "        ]\n",
    "        app_summary_map['Autoscale'] = autoscale_dict\n",
    "    else:\n",
    "        app_summary_map['Autoscale'] = 'Autoscale is not enabled.'\n",
    "    return app_summary_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  unravel_url = ''\n",
    "  unravel_token = ''\n",
    "  organization_url = ''\n",
    "  pat = ''\n",
    "  project_name = ''\n",
    "  build_id = ''\n",
    "  \n",
    "  try:\n",
    "    opts, args = getopt.getopt(sys.argv[1:], 'hu:t:o:a:p:b:',\n",
    "      ['unravel=', 'token=',  'organization=', 'pat=' , 'project=', 'build='])\n",
    "  except getopt.GetoptError:\n",
    "    print(\n",
    "      'azdevopsclient.py -u <unravel_url> -t <unravel_api_token>  -o <organization_url> -a <pat> -p <project_name> -b <build_id>)')\n",
    "    sys.exit(2)\n",
    "\n",
    "  for opt, arg in opts:\n",
    "    if opt == '-h':\n",
    "      print(\n",
    "        'azdevopsclient.py -u <unravel_url> -t <unravel_api_token>  -o <organization_url> -a <pat> -p <project_name> -b <build_id>)')\n",
    "      sys.exit()\n",
    "    elif opt in ('-u', '--unravel'):\n",
    "        unravel_url = arg\n",
    "    elif opt in ('-t', '--token'):\n",
    "        unravel_token = arg\n",
    "    elif opt in ('-o', '--organization'):\n",
    "        organization_url = arg\n",
    "    elif opt in ('-a', '--pat'):\n",
    "        pat = arg\n",
    "    elif opt in ('-p', '--project'):\n",
    "        project_name = arg\n",
    "    elif opt in ('-b', '--build'):\n",
    "        build_id = arg\n",
    "\n",
    "  print('-u : ' + unravel_url)\n",
    "  print('-t : ' + unravel_token)\n",
    "  print('-o : ' + organization_url)\n",
    "  print('-a : ' + pat)\n",
    "  print('-p : ' + project_name)\n",
    "  print('-b : ' + build_id)\n",
    "\n",
    "  # azure devops connection\n",
    "  connection = get_organization_connection(organization_url, pat)\n",
    "  #job_run_list = search_dbrks_job_runs(connection, project_name, build_id)\n",
    "\n",
    "\n",
    "  # get build\n",
    "  build = get_devops_build(connection, project_name, build_id)\n",
    "  build_json = build.as_dict()\n",
    "  \n",
    "  build_reason = build_json['reason']\n",
    "  if build_reason != 'pullRequest':\n",
    "     print(\"Nothing to do without PR\")\n",
    "     sys.exit(0)\n",
    "\n",
    "  pr_number = build_json['trigger_info']['pr.number']\n",
    "  repo_id = build_json['repository']['id']\n",
    "\n",
    "  print()\n",
    "  print(\"pr_number:\" + pr_number)\n",
    "  print(\"repo_id:\" + repo_id)\n",
    "\n",
    "  # get PR\n",
    "  pr = get_git_pr(connection, pr_number)\n",
    "  pr_json = pr.as_dict()\n",
    "  pr_id = pr_json['pull_request_id']\n",
    "  #description_json = json.loads(pr_json['description'])\n",
    "  #job_run_list = get_job_runs_from_description(pr_id, description_json)\n",
    "  raw_description = pr_json['description']\n",
    "  description = ' '.join(raw_description.splitlines())\n",
    "  description = re.sub(cleanRe, '', description)\n",
    "  job_run_list = get_job_runs_from_description_as_text(pr_id, description)\n",
    "\n",
    "  # start and end TS\n",
    "  today = datetime.today()\n",
    "  endDT = datetime(year=today.year, month=today.month, day=today.day, hour=today.hour, second=today.second) \n",
    "  startDT = endDT - timedelta(days=14)\n",
    "  start_time = startDT.astimezone().isoformat()\n",
    "  end_time = endDT.astimezone().isoformat()\n",
    "  print('start: ' + start_time)\n",
    "  print('end: '  + end_time)\n",
    "\n",
    "  job_run_result_list = []\n",
    "  for run in job_run_list:\n",
    "    gsp = run['workspace_id'] + '_' + run['job_id'] + '_' + run['run_id']\n",
    "    job_runs_json = search_summary_by_globalsearchpattern(unravel_url, unravel_token, start_time, end_time, gsp)\n",
    "    \n",
    "    if job_runs_json:\n",
    "      '''\n",
    "      gsp_file = gsp + '_summary.json'\n",
    "      with open(gsp_file, \"w\") as outfile:\n",
    "        json.dump(job_runs_json, outfile)\n",
    "      '''\n",
    "      clusterUId = job_runs_json[0]['clusterUid']\n",
    "      appId      = job_runs_json[0]['sparkAppId']\n",
    "      print(\"clusterUid: \" + clusterUId)\n",
    "      print(\"sparkAppId: \" + appId)\n",
    "\n",
    "      result_json = search_analysis(unravel_url, unravel_token, clusterUId, appId)\n",
    "      if result_json:\n",
    "        '''\n",
    "        gsp_file = gsp + '_analysis.json'\n",
    "        with open(gsp_file, \"w\") as outfile:\n",
    "          json.dump(result_json, outfile)\n",
    "        '''\n",
    "        insights_json = result_json['insightsV2']\n",
    "        recommendation_json = result_json['recommendation']\n",
    "        insights2_json = []\n",
    "        for item in insights_json:\n",
    "           #if item['key'] != 'SparkAppTimeReport':\n",
    "           insights2_json.append(item)\n",
    "        ''' \n",
    "        unravel_json = { 'workspace_id': run['workspace_id'],\n",
    "                         'job_id': run['job_id'],\n",
    "                         'run_id': run['run_id'],\n",
    "                         'dbrks_url': run['dbrks_url'],\n",
    "                         'unravel_url': unravel_url + '/#/jobs/runs',\n",
    "                         'unravel_keyword': gsp,\n",
    "                         'unravel_insights' : insights2_json,\n",
    "                         'unravel_recommendation': recommendation_json,\n",
    "        }\n",
    "        ''' \n",
    "        run['unravel_url'] = unravel_url + '/#/jobs/runs'\n",
    "        run['unravel_keyword'] = gsp\n",
    "        run['unravel_insights'] = insights2_json\n",
    "        run['unravel_recommendation'] = recommendation_json\n",
    "        run[\"app_summary\"] = fetch_app_summary(unravel_url, unravel_token, clusterUId, appId)\n",
    "        \n",
    "        # add to the list\n",
    "        job_run_result_list.append(run)\n",
    "    else:\n",
    "       print(\"job_run not found: \" + gsp)\n",
    "\n",
    "  '''  \n",
    "  gsp_file = 'cicd.json'\n",
    "  with open(gsp_file, \"w\") as outfile:\n",
    "    json.dump(job_run_result_list, outfile)\n",
    "  '''\n",
    "  if job_run_result_list:\n",
    "    '''\n",
    "    print(\"Current Description:\")\n",
    "    print(pr.description)\n",
    "    description_json['unravel'] = job_run_result_list\n",
    "    new_description = json.dumps(description_json)\n",
    "    clean_description = re.sub(cleanRe, '', new_description)\n",
    "    print(\"Updated Description:\")\n",
    "    print(clean_description)\n",
    "    new_pr = GitPullRequest()\n",
    "    new_pr.description = clean_description\n",
    "    update_git_pr(connection, new_pr, repo_id, pr_number, project_name)  \n",
    "    '''\n",
    "    #unravel_comments = re.sub(cleanRe, '', json.dumps(job_run_result_list, indent=4))\n",
    "    unravel_comments = create_comments_with_markdown(job_run_result_list)\n",
    "    comment_thread = CommentThread(\n",
    "      comments=[Comment(content=unravel_comments)],\n",
    "      status=\"Active\",\n",
    "    )\n",
    "    create_pr_comments(connection, repo_id, pr_number, comment_thread)\n",
    "  else:\n",
    "    print(\"Nothing to do without Unravel integration\")\n",
    "    sys.exit(0)\n",
    "     \n",
    "    \n",
    "    \n",
    "     \n",
    "                                   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-u : http://44.214.236.147:3000\n",
      "-t : JWT eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyb2xlIjoiYWRtaW4iLCJhcHBJZCI6ImNpY2QiLCJpZCI6IiIsInVzZXJuYW1lIjoiIiwidGFncyI6e30sImF1ZCI6IjNwIiwiaWF0IjoxNjgzNTY4NzE1fQ.Mthh9x832yQyJXOpQX6Ulj2b0CgUDLfWFoAjCAY67Jw\n",
      "-o : https://dev.azure.com/unravelcicdtest\n",
      "-a : i7zj4xhci332d6qm2qusuj22qydz7h5xnpuxgg5txp5t2r7oxoza\n",
      "-p : cicd\n",
      "-b : 28\n",
      "\n",
      "pr_number:8\n",
      "repo_id:7dbb1ce2-f239-4e62-a40d-ca1ebc894e27\n",
      "Description:\n",
      "\"https://adb-7575549084929882.2.azuredatabricks.net/?o=7575549084929882#job/867255411699781/run/26772 \", \"https://adb-7575549084929882.2.azuredatabricks.net/?o=7575549084929882#job/824732723619648/run/27000 \"\n",
      "Patten: https://adb-([0-9]+).([0-9]+).azuredatabricks.net/\\?o=([0-9]+)#job/([0-9]+)/run/([0-9]+)\n",
      "start: 2023-05-24T11:00:01+05:30\n",
      "end: 2023-06-07T11:00:01+05:30\n",
      "URL: http://44.214.236.147:3000/api/v1/ds/api/v1/databricks/runs/7575549084929882_867255411699781_26772/tasks/summary\n",
      "clusterUid: 0518-204437-vka2deom\n",
      "sparkAppId: local-1684442747797\n",
      "URL: http://44.214.236.147:3000/api/v1/spark/0518-204437-vka2deom/local-1684442747797/analysis\n",
      "URL: http://44.214.236.147:3000/api/v1/spark/0518-204437-vka2deom/local-1684442747797/appsummary\n",
      "URL: http://44.214.236.147:3000/api/v1/ds/api/v1/databricks/runs/7575549084929882_824732723619648_27000/tasks/summary\n",
      "clusterUid: 0518-205029-mnoiodcd\n",
      "sparkAppId: local-1684443056618\n",
      "URL: http://44.214.236.147:3000/api/v1/spark/0518-205029-mnoiodcd/local-1684443056618/analysis\n",
      "URL: http://44.214.236.147:3000/api/v1/spark/0518-205029-mnoiodcd/local-1684443056618/appsummary\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
